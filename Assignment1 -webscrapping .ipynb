{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b4a03f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\91939\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\91939\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\91939\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\91939\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91939\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\91939\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\91939\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91939\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e48028d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c95b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) Write a python program to display all the header tags from wikipedia.org.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "520d8072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all the header tags :\n",
      "\n",
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\"><span class=\"mw-page-title-main\">Main Page</span></h1>\n",
      "\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n",
      "\n",
      "<h2>Navigation menu</h2>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-personal-label\">\n",
      "<span class=\"vector-menu-heading-label\">Personal tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-namespaces-label\">\n",
      "<span class=\"vector-menu-heading-label\">Namespaces</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-views-label\">\n",
      "<span class=\"vector-menu-heading-label\">Views</span>\n",
      "</h3>\n",
      "\n",
      "<h3>\n",
      "<label for=\"searchInput\">Search</label>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-navigation-label\">\n",
      "<span class=\"vector-menu-heading-label\">Navigation</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-interaction-label\">\n",
      "<span class=\"vector-menu-heading-label\">Contribute</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-tb-label\">\n",
      "<span class=\"vector-menu-heading-label\">Tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-coll-print_export-label\">\n",
      "<span class=\"vector-menu-heading-label\">Print/export</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-wikibase-otherprojects-label\">\n",
      "<span class=\"vector-menu-heading-label\">In other projects</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-lang-label\">\n",
      "<span class=\"vector-menu-heading-label\">Languages</span>\n",
      "</h3>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "print('List all the header tags :', *titles, sep='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec11a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release)\n",
    "#and make dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b0e94c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB’s Top rated 100 movies' data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year of Release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pulp Fiction</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Inception</td>\n",
       "      <td>8.8</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Lord of the Rings: The Two Towers</td>\n",
       "      <td>8.8</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fight Club</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>8.8</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Forrest Gump</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Il buono, il brutto, il cattivo</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Matrix</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Goodfellas</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The Empire Strikes Back</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>One Flew Over the Cuckoo's Nest</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Interstellar</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Cidade de Deus</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sen to Chihiro no kamikakushi</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Saving Private Ryan</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The Green Mile</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>La vita è bella</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Se7en</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Terminator 2: Judgment Day</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>The Silence of the Lambs</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Star Wars</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Seppuku</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Shichinin no samurai</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>It's a Wonderful Life</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Gisaengchung</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Whiplash</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>The Intouchables</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>The Prestige</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The Departed</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>The Pianist</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Gladiator</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>American History X</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The Usual Suspects</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Léon</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>The Lion King</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Nuovo Cinema Paradiso</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Hotaru no haka</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Back to the Future</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Apocalypse Now</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Alien</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Once Upon a Time in the West</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Psycho</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Rear Window</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Casablanca</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Modern Times</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>City Lights</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Capharnaüm</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Joker (I)</td>\n",
       "      <td>8.4</td>\n",
       "      <td>I 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Kimi no na wa.</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Spider-Man: Into the Spider-Verse</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Avengers: Endgame</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Avengers: Infinity War</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Coco (I)</td>\n",
       "      <td>8.4</td>\n",
       "      <td>I 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Django Unchained</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Top Gun: Maverick</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>3 Idiots</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>WALL·E</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>The Lives of Others</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Oldeuboi</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Memento</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>American Beauty</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Mononoke-hime</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Braveheart</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Idi i smotri</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Aliens</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Amadeus</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Raiders of the Lost Ark</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Das Boot</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>The Shining</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Tengoku to jigoku</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Dr. Strangelove or: How I Learned to Stop Worr...</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Witness for the Prosecution</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Paths of Glory</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Sunset Blvd.</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>The Great Dictator</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Jagten</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Toy Story 3</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Inglourious Basterds</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Eternal Sunshine of the Spotless Mind</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Requiem for a Dream</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Good Will Hunting</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Toy Story</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Reservoir Dogs</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Once Upon a Time in America</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Star Wars: Episode VI - Return of the Jedi</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2001: A Space Odyssey</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Lawrence of Arabia</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Vertigo</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Singin' in the Rain</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Citizen Kane</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>M - Eine Stadt sucht einen Mörder</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Name Rating Year of Release\n",
       "1                             The Shawshank Redemption    9.3            1994\n",
       "2                                        The Godfather    9.2            1972\n",
       "3                                      The Dark Knight    9.0            2008\n",
       "4        The Lord of the Rings: The Return of the King    9.0            2003\n",
       "5                                     Schindler's List    9.0            1993\n",
       "6                                The Godfather Part II    9.0            1974\n",
       "7                                         12 Angry Men    9.0            1957\n",
       "8                                         Pulp Fiction    8.9            1994\n",
       "9                                            Inception    8.8            2010\n",
       "10               The Lord of the Rings: The Two Towers    8.8            2002\n",
       "11                                          Fight Club    8.8            1999\n",
       "12   The Lord of the Rings: The Fellowship of the Ring    8.8            2001\n",
       "13                                        Forrest Gump    8.8            1994\n",
       "14                     Il buono, il brutto, il cattivo    8.8            1966\n",
       "15                                          The Matrix    8.7            1999\n",
       "16                                          Goodfellas    8.7            1990\n",
       "17                             The Empire Strikes Back    8.7            1980\n",
       "18                     One Flew Over the Cuckoo's Nest    8.7            1975\n",
       "19                                        Interstellar    8.6            2014\n",
       "20                                      Cidade de Deus    8.6            2002\n",
       "21                       Sen to Chihiro no kamikakushi    8.6            2001\n",
       "22                                 Saving Private Ryan    8.6            1998\n",
       "23                                      The Green Mile    8.6            1999\n",
       "24                                     La vita è bella    8.6            1997\n",
       "25                                               Se7en    8.6            1995\n",
       "26                          Terminator 2: Judgment Day    8.6            1991\n",
       "27                            The Silence of the Lambs    8.6            1991\n",
       "28                                           Star Wars    8.6            1977\n",
       "29                                             Seppuku    8.6            1962\n",
       "30                                Shichinin no samurai    8.6            1954\n",
       "31                               It's a Wonderful Life    8.6            1946\n",
       "32                                        Gisaengchung    8.5            2019\n",
       "33                                            Whiplash    8.5            2014\n",
       "34                                    The Intouchables    8.5            2011\n",
       "35                                        The Prestige    8.5            2006\n",
       "36                                        The Departed    8.5            2006\n",
       "37                                         The Pianist    8.5            2002\n",
       "38                                           Gladiator    8.5            2000\n",
       "39                                  American History X    8.5            1998\n",
       "40                                  The Usual Suspects    8.5            1995\n",
       "41                                                Léon    8.5            1994\n",
       "42                                       The Lion King    8.5            1994\n",
       "43                               Nuovo Cinema Paradiso    8.5            1988\n",
       "44                                      Hotaru no haka    8.5            1988\n",
       "45                                  Back to the Future    8.5            1985\n",
       "46                                      Apocalypse Now    8.5            1979\n",
       "47                                               Alien    8.5            1979\n",
       "48                        Once Upon a Time in the West    8.5            1968\n",
       "49                                              Psycho    8.5            1960\n",
       "50                                         Rear Window    8.5            1954\n",
       "51                                          Casablanca    8.5            1942\n",
       "52                                        Modern Times    8.5            1936\n",
       "53                                         City Lights    8.5            1931\n",
       "54                                          Capharnaüm    8.4            2018\n",
       "55                                           Joker (I)    8.4          I 2019\n",
       "56                                      Kimi no na wa.    8.4            2016\n",
       "57                   Spider-Man: Into the Spider-Verse    8.4            2018\n",
       "58                                   Avengers: Endgame    8.4            2019\n",
       "59                              Avengers: Infinity War    8.4            2018\n",
       "60                                            Coco (I)    8.4          I 2017\n",
       "61                                    Django Unchained    8.4            2012\n",
       "62                                   Top Gun: Maverick    8.4            2022\n",
       "63                               The Dark Knight Rises    8.4            2012\n",
       "64                                            3 Idiots    8.4            2009\n",
       "65                                              WALL·E    8.4            2008\n",
       "66                                 The Lives of Others    8.4            2006\n",
       "67                                            Oldeuboi    8.4            2003\n",
       "68                                             Memento    8.4            2000\n",
       "69                                     American Beauty    8.4            1999\n",
       "70                                       Mononoke-hime    8.4            1997\n",
       "71                                          Braveheart    8.4            1995\n",
       "72                                        Idi i smotri    8.4            1985\n",
       "73                                              Aliens    8.4            1986\n",
       "74                                             Amadeus    8.4            1984\n",
       "75                             Raiders of the Lost Ark    8.4            1981\n",
       "76                                            Das Boot    8.4            1981\n",
       "77                                         The Shining    8.4            1980\n",
       "78                                   Tengoku to jigoku    8.4            1963\n",
       "79   Dr. Strangelove or: How I Learned to Stop Worr...    8.4            1964\n",
       "80                         Witness for the Prosecution    8.4            1957\n",
       "81                                      Paths of Glory    8.4            1957\n",
       "82                                        Sunset Blvd.    8.4            1950\n",
       "83                                  The Great Dictator    8.4            1940\n",
       "84                                              Jagten    8.3            2012\n",
       "85                                         Toy Story 3    8.3            2010\n",
       "86                                Inglourious Basterds    8.3            2009\n",
       "87               Eternal Sunshine of the Spotless Mind    8.3            2004\n",
       "88                                 Requiem for a Dream    8.3            2000\n",
       "89                                   Good Will Hunting    8.3            1997\n",
       "90                                           Toy Story    8.3            1995\n",
       "91                                      Reservoir Dogs    8.3            1992\n",
       "92                         Once Upon a Time in America    8.3            1984\n",
       "93          Star Wars: Episode VI - Return of the Jedi    8.3            1983\n",
       "94                               2001: A Space Odyssey    8.3            1968\n",
       "95                                  Lawrence of Arabia    8.3            1962\n",
       "96                                  North by Northwest    8.3            1959\n",
       "97                                             Vertigo    8.3            1958\n",
       "98                                 Singin' in the Rain    8.3            1952\n",
       "99                                        Citizen Kane    8.3            1941\n",
       "100                  M - Eine Stadt sucht einen Mörder    8.3            1931"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    " \n",
    " \n",
    "page2=requests.get(\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc\")  #Contains top 50 movies\n",
    "page2a=requests.get(\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&start=51&ref_=adv_nxt\")    #Contains corresponding top 50 movies\n",
    "\n",
    "page2          #As the response is 200, we can proceed with this url\n",
    "page2a         #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup2= BeautifulSoup(page2.content)\n",
    "soup2a= BeautifulSoup(page2a.content)\n",
    "\n",
    "name=[]            #Empty list to store movie names\n",
    "for i in soup2.find_all('h3',class_=\"lister-item-header\"):\n",
    "    l=len(i.text.split())\n",
    "    name.append(' '.join(i.text.split()[1:l-1]))          #To store only the movie name as a string\n",
    "for i in soup2a.find_all('h3',class_=\"lister-item-header\"):\n",
    "    l=len(i.text.split())\n",
    "    name.append(' '.join(i.text.split()[1:l-1]))\n",
    "\n",
    "rating=[]         #Empty list to store rating\n",
    "for i in soup2.find_all('div',class_=\"inline-block ratings-imdb-rating\"):\n",
    "    rating.append(' '.join(i.text.split()))               #To store only the rating as a string\n",
    "for i in soup2a.find_all('div',class_=\"inline-block ratings-imdb-rating\"):\n",
    "    rating.append(' '.join(i.text.split()))\n",
    "\n",
    "year=[]           #Empty list to store year of release\n",
    "for i in soup2.find_all('span',class_=\"lister-item-year text-muted unbold\"):\n",
    "    year.append(i.text.replace('(','').replace(')',''))    #To store only the year of release as a string\n",
    "for i in soup2a.find_all('span',class_=\"lister-item-year text-muted unbold\"):\n",
    "    year.append(i.text.replace('(','').replace(')',''))\n",
    "    \n",
    "s_no=[]\n",
    "for j in range(1,101):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)                #To display the entire dataframe\n",
    "df=pd.DataFrame({'Name':name, 'Rating':rating, 'Year of Release': year}, index=s_no)\n",
    "print(\"IMDB’s Top rated 100 movies' data\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93d05197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB’s Top rated 100 movies' data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year of Release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kantara</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ramayana: The Legend of Prince Rama</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rocketry: The Nambi Effect</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nayakan</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anbe Sivam</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Golmaal</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jai Bhim</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>777 Charlie</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pariyerum Perumal</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3 Idiots</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Apur Sansar</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Manichitrathazhu</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Black Friday</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kumbalangi Nights</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Soorarai Pottru</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>#Home</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Taare Zameen Par</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>C/o Kancharapalem</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Kireedam</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Dangal</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Kaithi</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Jersey</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>96</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Asuran</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Thevar Magan</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Visaaranai</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Natsamrat</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Drishyam 2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Thalapathi</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Pather Panchali</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Sarpatta Parambarai</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Jaane Bhi Do Yaaro</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Thani Oruvan</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Sardar Udham</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Aparajito</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Drishyam</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Vada Chennai</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Khosla Ka Ghosla!</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Sita Ramam</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Ratsasan</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Chupke Chupke</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Anniyan</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Peranbu</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Satya</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Mahanati</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Gangs of Wasseypur</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Premam</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Bangalore Days</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Agent Sai Srinivasa Athreya</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Super Deluxe</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Devasuram</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Drishyam</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Bhaag Milkha Bhaag</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Vikram Vedha</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Vikram</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Andhadhun</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Guide</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Tumbbad</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Kannathil Muthamittal</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Aruvi</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Chithram</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Zindagi Na Milegi Dobara</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Shahid</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Sairat</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Iruvar</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Paan Singh Tomar</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Chhichhore</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Swades: We, the People</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Pyaasa</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Chak De! India</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Mudhalvan</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Munna Bhai M.B.B.S.</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Black</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Uri: The Surgical Strike</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Spadikam</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Jo Jeeta Wohi Sikandar</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Pudhu Pettai</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Dhuruvangal Pathinaaru</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Papanasam</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Lagaan: Once Upon a Time in India</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Queen</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Article 15</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Talvar</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Hera Pheri</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Mandela</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>PK</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Soodhu Kavvum</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>OMG: Oh My God!</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Sarfarosh</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Sholay</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Udaan</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Kaakkaa Muttai</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Jigarthanda</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Barfi!</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>The Legend of Bhagat Singh</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Ustad Hotel</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Theeran Adhigaaram Ondru</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Rang De Basanti</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Baahubali 2: The Conclusion</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Angoor</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Name Rating Year of Release\n",
       "1                                Kantara    8.5            2022\n",
       "2    Ramayana: The Legend of Prince Rama    8.5            1993\n",
       "3             Rocketry: The Nambi Effect    8.4            2022\n",
       "4                                Nayakan    8.4            1987\n",
       "5                             Anbe Sivam    8.4            2003\n",
       "6                                Golmaal    8.4            1979\n",
       "7                               Jai Bhim    8.4            2021\n",
       "8                            777 Charlie    8.4            2022\n",
       "9                      Pariyerum Perumal    8.4            2018\n",
       "10                              3 Idiots    8.4            2009\n",
       "11                           Apur Sansar    8.3            1959\n",
       "12                      Manichitrathazhu    8.3            1993\n",
       "13                          Black Friday    8.3            2004\n",
       "14                     Kumbalangi Nights    8.3            2019\n",
       "15                       Soorarai Pottru    8.3            2020\n",
       "16                                 #Home    8.3            2021\n",
       "17                      Taare Zameen Par    8.3            2007\n",
       "18                     C/o Kancharapalem    8.3            2018\n",
       "19                              Kireedam    8.3            1989\n",
       "20                                Dangal    8.3            2016\n",
       "21                                Kaithi    8.3            2019\n",
       "22                                Jersey    8.3            2019\n",
       "23                                    96    8.2            2018\n",
       "24                                Asuran    8.2            2019\n",
       "25                          Thevar Magan    8.2            1992\n",
       "26                            Visaaranai    8.2            2015\n",
       "27                             Natsamrat    8.2            2016\n",
       "28                            Drishyam 2    8.2            2021\n",
       "29                            Thalapathi    8.2            1991\n",
       "30                       Pather Panchali    8.2            1955\n",
       "31                   Sarpatta Parambarai    8.2            2021\n",
       "32                    Jaane Bhi Do Yaaro    8.2            1983\n",
       "33                          Thani Oruvan    8.2            2015\n",
       "34                          Sardar Udham    8.2            2021\n",
       "35                             Aparajito    8.2            1956\n",
       "36                              Drishyam    8.2            2013\n",
       "37                          Vada Chennai    8.2            2018\n",
       "38                     Khosla Ka Ghosla!    8.2            2006\n",
       "39                            Sita Ramam    8.2            2022\n",
       "40                              Ratsasan    8.1            2018\n",
       "41                         Chupke Chupke    8.1            1975\n",
       "42                               Anniyan    8.1            2005\n",
       "43                               Peranbu    8.1            2018\n",
       "44                                 Satya    8.1            1998\n",
       "45                              Mahanati    8.1            2018\n",
       "46                    Gangs of Wasseypur    8.1            2012\n",
       "47                                Premam    8.1            2015\n",
       "48                        Bangalore Days    8.1            2014\n",
       "49           Agent Sai Srinivasa Athreya    8.1            2019\n",
       "50                          Super Deluxe    8.1            2019\n",
       "51                             Devasuram    8.1            1993\n",
       "52                              Drishyam    8.1            2015\n",
       "53                    Bhaag Milkha Bhaag    8.1            2013\n",
       "54                          Vikram Vedha    8.1            2017\n",
       "55                                Vikram    8.1            2022\n",
       "56                             Andhadhun    8.1            2018\n",
       "57                                 Guide    8.1            1965\n",
       "58                               Tumbbad    8.1            2018\n",
       "59                 Kannathil Muthamittal    8.1            2002\n",
       "60                                 Aruvi    8.1            2016\n",
       "61                              Chithram    8.1            1988\n",
       "62              Zindagi Na Milegi Dobara    8.1            2011\n",
       "63                                Shahid    8.1            2012\n",
       "64                                Sairat    8.1            2016\n",
       "65                                Iruvar    8.1            1997\n",
       "66                      Paan Singh Tomar    8.1            2012\n",
       "67                            Chhichhore    8.1            2019\n",
       "68                Swades: We, the People    8.1            2004\n",
       "69                                Pyaasa    8.1            1957\n",
       "70                        Chak De! India    8.1            2007\n",
       "71                             Mudhalvan    8.1            1999\n",
       "72                   Munna Bhai M.B.B.S.    8.1            2003\n",
       "73                                 Black    8.1            2005\n",
       "74              Uri: The Surgical Strike    8.1            2019\n",
       "75                              Spadikam    8.1            1995\n",
       "76                Jo Jeeta Wohi Sikandar    8.1            1992\n",
       "77                          Pudhu Pettai    8.1            2006\n",
       "78                Dhuruvangal Pathinaaru    8.1            2016\n",
       "79                             Papanasam    8.1            2015\n",
       "80     Lagaan: Once Upon a Time in India    8.0            2001\n",
       "81                                 Queen    8.0            2013\n",
       "82                            Article 15    8.0            2019\n",
       "83                                Talvar    8.0            2015\n",
       "84                            Hera Pheri    8.0            2000\n",
       "85                               Mandela    8.0            2021\n",
       "86                                    PK    8.0            2014\n",
       "87                         Soodhu Kavvum    8.0            2013\n",
       "88                       OMG: Oh My God!    8.0            2012\n",
       "89                             Sarfarosh    8.0            1999\n",
       "90                                Sholay    8.0            1975\n",
       "91                                 Udaan    8.0            2010\n",
       "92                        Kaakkaa Muttai    8.0            2014\n",
       "93                           Jigarthanda    8.0            2014\n",
       "94                                Barfi!    8.0            2012\n",
       "95            The Legend of Bhagat Singh    8.0            2002\n",
       "96                           Ustad Hotel    8.0            2012\n",
       "97              Theeran Adhigaaram Ondru    8.0            2017\n",
       "98                       Rang De Basanti    8.0            2006\n",
       "99           Baahubali 2: The Conclusion    8.0            2017\n",
       "100                               Angoor    8.0            1982"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#3) Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and storing  in dataframe\n",
    "\n",
    "\n",
    "#https://www.imdb.com/india/top-rated-indian-movies/\n",
    "\n",
    "page=requests.get(\"https://www.imdb.com/india/top-rated-indian-movies/?sort=ir,desc&mode=simple&page=1\")  \n",
    "\n",
    "page                 #As the response is 200, we can proceed with this url\n",
    "             \n",
    "soup= BeautifulSoup(page3.content)\n",
    "\n",
    "a=1             #counters to keep top 100 indian movies\n",
    "b=1\n",
    "c=1\n",
    "\n",
    "name=[]            #Empty list to store movie names\n",
    "for i in soup.find_all('td',class_=\"titleColumn\"):\n",
    "    l=len(i.text.split())\n",
    "    if a<=100:                                                #To store top 100 movies\n",
    "        name.append(' '.join(i.text.split()[1:l-1]))          #To store only the movie name as a string\n",
    "        a=a+1\n",
    "\n",
    "rating=[]         #Empty list to store rating\n",
    "for i in soup.find_all('td',class_=\"ratingColumn imdbRating\"):\n",
    "    if b<=100:\n",
    "        rating.append(' '.join(i.text.split()))               #To store only the rating as a string\n",
    "        b=b+1\n",
    "\n",
    "year=[]           #Empty list to store year of release\n",
    "for i in soup.find_all('span',class_=\"secondaryInfo\"):\n",
    "    if c<=100:\n",
    "        year.append(i.text.replace('(','').replace(')',''))    #To store only the year of release as a string\n",
    "        c=c+1\n",
    "        \n",
    "s_no=[]\n",
    "for j in range(1,101):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)                #To display the entire dataframe\n",
    "df=pd.DataFrame({'Name':name, 'Rating':rating, 'Year of Release': year}, index=s_no)\n",
    "print(\"IMDB’s Top rated 100 movies' data\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ac2ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16d17ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of respected former presidents of India\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term of Office</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Shri Ram Nath Kovind</th>\n",
       "      <td>25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shri Pranab Mukherjee</th>\n",
       "      <td>25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smt Pratibha Devisingh Patil</th>\n",
       "      <td>25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DR. A.P.J. Abdul Kalam</th>\n",
       "      <td>25 July, 2002 to 25 July, 2007 http://abdulkal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shri K. R. Narayanan</th>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dr Shankar Dayal Sharma</th>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shri R Venkataraman</th>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Giani Zail Singh</th>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shri Neelam Sanjiva Reddy</th>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dr. Fakhruddin Ali Ahmed</th>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shri Varahagiri Venkata Giri</th>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dr. Zakir Husain</th>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dr. Sarvepalli Radhakrishnan</th>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dr. Rajendra Prasad</th>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 Term of Office\n",
       "Name                                                                           \n",
       "Shri Ram Nath Kovind                             25 July, 2017 to 25 July, 2022\n",
       "Shri Pranab Mukherjee                            25 July, 2012 to 25 July, 2017\n",
       "Smt Pratibha Devisingh Patil                     25 July, 2007 to 25 July, 2012\n",
       "DR. A.P.J. Abdul Kalam        25 July, 2002 to 25 July, 2007 http://abdulkal...\n",
       "Shri K. R. Narayanan                             25 July, 1997 to 25 July, 2002\n",
       "Dr Shankar Dayal Sharma                          25 July, 1992 to 25 July, 1997\n",
       "Shri R Venkataraman                              25 July, 1987 to 25 July, 1992\n",
       "Giani Zail Singh                                 25 July, 1982 to 25 July, 1987\n",
       "Shri Neelam Sanjiva Reddy                        25 July, 1977 to 25 July, 1982\n",
       "Dr. Fakhruddin Ali Ahmed                   24 August, 1974 to 11 February, 1977\n",
       "Shri Varahagiri Venkata Giri  3 May, 1969 to 20 July, 1969 and 24 August, 19...\n",
       "Dr. Zakir Husain                                    13 May, 1967 to 3 May, 1969\n",
       "Dr. Sarvepalli Radhakrishnan                       13 May, 1962 to 13 May, 1967\n",
       "Dr. Rajendra Prasad                            26 January, 1950 to 13 May, 1962"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4) Write s python program to display list of respected former presidents of India(i.e. Name , Term of office) \n",
    "\n",
    "#https://presidentofindia.nic.in/former-presidents.html\n",
    "\n",
    "page=requests.get(\"https://presidentofindia.nic.in/former-presidents.htm\")\n",
    "page           #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup= BeautifulSoup(page4.content)\n",
    "\n",
    "pres=[]                   #Empty list to store names of respected former presidents of India\n",
    "for i in soup.find_all('div',class_=\"presidentListing\"):\n",
    "    l=len(i.text.split())\n",
    "    for j in range(0,l):\n",
    "        a=i.text.split()[j]\n",
    "        if a[0]=='(':\n",
    "            k=j\n",
    "            break\n",
    "    pres.append(' '.join(i.text.split()[0:k]))          #To store only the names of respected former presidents of India\n",
    "\n",
    "tf=[]\n",
    "z=1\n",
    "for i in soup.find_all('div',class_=\"presidentListing\"):\n",
    "    l=len(i.text.split())\n",
    "    for j in range(0,l):\n",
    "        a=i.text.split()[j]\n",
    "        if a=='Office:':\n",
    "            k=j\n",
    "            break\n",
    "    if z<=3:\n",
    "        tf.append(' '.join(i.text.split()[k+1:l-1]))    #To store only the term of office of respected former presidents of India\n",
    "        z=z+1\n",
    "    else:\n",
    "        tf.append(' '.join(i.text.split()[k+1:l])) \n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df=pd.DataFrame({'Name':pres, 'Term of Office':tf})\n",
    "print(\"The list of respected former presidents of India\")\n",
    "df.set_index('Name', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d6aed",
   "metadata": {},
   "source": [
    "Q7# Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :) Headline\n",
    "ii) Time\n",
    "iii) News Link\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f641bd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News details\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>News links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Russia halts participation in an agreement to ...</td>\n",
       "      <td>9 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/russia-halts-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brooklyn Nets condemn Kyrie Irving for promoti...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/brooklyn-nets-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>These dads drove cross country, raised $156,00...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/dads-road-trip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How 'The 5 Love Languages' stays relevant 30 y...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/how-the-5-love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I raised 2 successful CEOs and a doctor. Here'...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/i-raised-2-suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Here's a look at some legal and personal ramif...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/here-are-the-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Issa Rae shares her best career advice and how...</td>\n",
       "      <td>13 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/issa-rae-share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>These people are making real money in Horizon ...</td>\n",
       "      <td>13 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/metaverse-entr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What 'millionaire tax' plans on California, Ma...</td>\n",
       "      <td>13 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/what-millionai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A 35-year-old cult classic from John Carpenter...</td>\n",
       "      <td>13 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/john-carpenter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>From Chipotle to Wendy’s, here are 7 places to...</td>\n",
       "      <td>13 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/where-to-score...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Twitter is now owned by Elon Musk — here's a b...</td>\n",
       "      <td>14 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/a-brief-histor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The market is moving in-line with seasonal pat...</td>\n",
       "      <td>14 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/the-market-is-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Stocks reporting earnings in the week ahead th...</td>\n",
       "      <td>14 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/earnings-resul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Goldman Sachs analysts say these are the best ...</td>\n",
       "      <td>14 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/goldman-sachs-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Derek and Hannah Jeter sign multiyear deal wit...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/derek-jeter-pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Cramer on Friday named travel as one of five r...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/jim-cramer-say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cramer's lightning round: Let's stay with Fron...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/cramers-lightn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Cramer's week ahead: There could be 'real sign...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/cramers-week-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Co-defendant in SEC civil fraud suit against f...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/fake-billionai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>UN details horrifying accounts of rape, tortur...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/russia-ukraine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Stocks shrug off tech's troubles as Street awa...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/stocks-shrugge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Pro Picks: Watch all of Friday's big stock cal...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/pro-picks-watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GM temporarily suspends advertising on Twitter...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/gm-temporarily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Big Tech falters on dreary earnings — Meta has...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/big-tech-falte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>93% of daters want a partner who is emotionall...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/how-to-be-emot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Omicron subvariants resistant to key antibody ...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/omicron-subvar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Jeep parent Stellantis offering buyouts to som...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/chrysler-owner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Musk plans Twitter content moderation council ...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/musk-plans-twi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Why Bryan Cranston, Aaron Paul work 17-hour da...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/bryan-cranston...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline              Time  \\\n",
       "1   Russia halts participation in an agreement to ...       9 Hours Ago   \n",
       "2   Brooklyn Nets condemn Kyrie Irving for promoti...      11 Hours Ago   \n",
       "3   These dads drove cross country, raised $156,00...      11 Hours Ago   \n",
       "4   How 'The 5 Love Languages' stays relevant 30 y...      12 Hours Ago   \n",
       "5   I raised 2 successful CEOs and a doctor. Here'...      12 Hours Ago   \n",
       "6   Here's a look at some legal and personal ramif...      12 Hours Ago   \n",
       "7   Issa Rae shares her best career advice and how...      13 Hours Ago   \n",
       "8   These people are making real money in Horizon ...      13 Hours Ago   \n",
       "9   What 'millionaire tax' plans on California, Ma...      13 Hours Ago   \n",
       "10  A 35-year-old cult classic from John Carpenter...      13 Hours Ago   \n",
       "11  From Chipotle to Wendy’s, here are 7 places to...      13 Hours Ago   \n",
       "12  Twitter is now owned by Elon Musk — here's a b...      14 Hours Ago   \n",
       "13  The market is moving in-line with seasonal pat...      14 Hours Ago   \n",
       "14  Stocks reporting earnings in the week ahead th...      14 Hours Ago   \n",
       "15  Goldman Sachs analysts say these are the best ...      14 Hours Ago   \n",
       "16  Derek and Hannah Jeter sign multiyear deal wit...  October 28, 2022   \n",
       "17  Cramer on Friday named travel as one of five r...  October 28, 2022   \n",
       "18  Cramer's lightning round: Let's stay with Fron...  October 28, 2022   \n",
       "19  Cramer's week ahead: There could be 'real sign...  October 28, 2022   \n",
       "20  Co-defendant in SEC civil fraud suit against f...  October 28, 2022   \n",
       "21  UN details horrifying accounts of rape, tortur...  October 28, 2022   \n",
       "22  Stocks shrug off tech's troubles as Street awa...  October 28, 2022   \n",
       "23  Pro Picks: Watch all of Friday's big stock cal...  October 28, 2022   \n",
       "24  GM temporarily suspends advertising on Twitter...  October 28, 2022   \n",
       "25  Big Tech falters on dreary earnings — Meta has...  October 28, 2022   \n",
       "26  93% of daters want a partner who is emotionall...  October 28, 2022   \n",
       "27  Omicron subvariants resistant to key antibody ...  October 28, 2022   \n",
       "28  Jeep parent Stellantis offering buyouts to som...  October 28, 2022   \n",
       "29  Musk plans Twitter content moderation council ...  October 28, 2022   \n",
       "30  Why Bryan Cranston, Aaron Paul work 17-hour da...  October 28, 2022   \n",
       "\n",
       "                                           News links  \n",
       "1   https://www.cnbc.com/2022/10/29/russia-halts-p...  \n",
       "2   https://www.cnbc.com/2022/10/29/brooklyn-nets-...  \n",
       "3   https://www.cnbc.com/2022/10/29/dads-road-trip...  \n",
       "4   https://www.cnbc.com/2022/10/29/how-the-5-love...  \n",
       "5   https://www.cnbc.com/2022/10/29/i-raised-2-suc...  \n",
       "6   https://www.cnbc.com/2022/10/29/here-are-the-l...  \n",
       "7   https://www.cnbc.com/2022/10/29/issa-rae-share...  \n",
       "8   https://www.cnbc.com/2022/10/29/metaverse-entr...  \n",
       "9   https://www.cnbc.com/2022/10/29/what-millionai...  \n",
       "10  https://www.cnbc.com/2022/10/29/john-carpenter...  \n",
       "11  https://www.cnbc.com/2022/10/29/where-to-score...  \n",
       "12  https://www.cnbc.com/2022/10/29/a-brief-histor...  \n",
       "13  https://www.cnbc.com/2022/10/29/the-market-is-...  \n",
       "14  https://www.cnbc.com/2022/10/29/earnings-resul...  \n",
       "15  https://www.cnbc.com/2022/10/29/goldman-sachs-...  \n",
       "16  https://www.cnbc.com/2022/10/28/derek-jeter-pr...  \n",
       "17  https://www.cnbc.com/2022/10/28/jim-cramer-say...  \n",
       "18  https://www.cnbc.com/2022/10/28/cramers-lightn...  \n",
       "19  https://www.cnbc.com/2022/10/28/cramers-week-a...  \n",
       "20  https://www.cnbc.com/2022/10/28/fake-billionai...  \n",
       "21  https://www.cnbc.com/2022/10/28/russia-ukraine...  \n",
       "22  https://www.cnbc.com/2022/10/28/stocks-shrugge...  \n",
       "23  https://www.cnbc.com/2022/10/28/pro-picks-watc...  \n",
       "24  https://www.cnbc.com/2022/10/28/gm-temporarily...  \n",
       "25  https://www.cnbc.com/2022/10/28/big-tech-falte...  \n",
       "26  https://www.cnbc.com/2022/10/28/how-to-be-emot...  \n",
       "27  https://www.cnbc.com/2022/10/28/omicron-subvar...  \n",
       "28  https://www.cnbc.com/2022/10/28/chrysler-owner...  \n",
       "29  https://www.cnbc.com/2022/10/28/musk-plans-twi...  \n",
       "30  https://www.cnbc.com/2022/10/28/bryan-cranston...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get(\"https://www.cnbc.com/world/?region=world\")\n",
    "page   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup= BeautifulSoup(page7.content)\n",
    "\n",
    "headline=[]                 #To store headlines\n",
    "for i in soup.find_all('a',class_=\"LatestNews-headline\"):\n",
    "    headline.append(i.get('title'))\n",
    "    \n",
    "time=[]                    #To store time\n",
    "for i in soup.find_all('time',class_=\"LatestNews-timestamp\"):\n",
    "    time.append(i.text)\n",
    "\n",
    "url=[]                   #To store the urls\n",
    "for i in soup.find_all('a',class_=\"LatestNews-headline\"):\n",
    "    url.append(i.get('href'))\n",
    "\n",
    "l=len(headline)\n",
    "s_no=[]\n",
    "for j in range(1,l+1):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df=pd.DataFrame({'Headline':headline, 'Time':time, 'News links':url}, index=s_no)\n",
    "print(\"News details\")\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6574aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The details of most downloaded articles from AI in last 90 days\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Titles</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Date of Publication</th>\n",
       "      <th>Paper Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Paper Titles  \\\n",
       "1                                    Reward is enough   \n",
       "2                           Making sense of raw input   \n",
       "3   Law and logic: A review from an argumentation ...   \n",
       "4              Creativity and artificial intelligence   \n",
       "5   Artificial cognition for social human–robot in...   \n",
       "6   Explanation in artificial intelligence: Insigh...   \n",
       "7                       Making sense of sensory input   \n",
       "8   Conflict-based search for optimal multi-agent ...   \n",
       "9   Between MDPs and semi-MDPs: A framework for te...   \n",
       "10  The Hanabi challenge: A new frontier for AI re...   \n",
       "11  Evaluating XAI: A comparison of rule-based and...   \n",
       "12           Argumentation in artificial intelligence   \n",
       "13  Algorithms for computing strategies in two-pla...   \n",
       "14      Multiple object tracking: A literature review   \n",
       "15  Selection of relevant features and examples in...   \n",
       "16  A survey of inverse reinforcement learning: Ch...   \n",
       "17  Explaining individual predictions when feature...   \n",
       "18  A review of possible effects of cognitive bias...   \n",
       "19  Integrating social power into the decision-mak...   \n",
       "20  “That's (not) the output I expected!” On the r...   \n",
       "21  Explaining black-box classifiers using post-ho...   \n",
       "22  Algorithm runtime prediction: Methods & evalua...   \n",
       "23              Wrappers for feature subset selection   \n",
       "24  Commonsense visual sensemaking for autonomous ...   \n",
       "25         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                              Authors Date of Publication  \\\n",
       "1   Silver, David, Singh, Satinder, Precup, Doina,...        October 2021   \n",
       "2           Evans, Richard, Bošnjak, Matko and 5 more        October 2021   \n",
       "3                   Prakken, Henry, Sartor, Giovanni         October 2015   \n",
       "4                                 Boden, Margaret A.          August 1998   \n",
       "5     Lemaignan, Séverin, Warnier, Mathieu and 3 more           June 2017   \n",
       "6                                        Miller, Tim        February 2019   \n",
       "7   Evans, Richard, Hernández-Orallo, José and 3 more          April 2021   \n",
       "8   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...       February 2015   \n",
       "9   Sutton, Richard S., Precup, Doina, Singh, Sati...         August 1999   \n",
       "10        Bard, Nolan, Foerster, Jakob N. and 13 more          March 2020   \n",
       "11  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...       February 2021   \n",
       "12               Bench-Capon, T.J.M., Dunne, Paul E.         October 2007   \n",
       "13       Bošanský, Branislav, Lisý, Viliam and 3 more         August 2016   \n",
       "14             Luo, Wenhan, Xing, Junliang and 4 more          April 2021   \n",
       "15                      Blum, Avrim L., Langley, Pat        December 1997   \n",
       "16                   Arora, Saurabh, Doshi, Prashant          August 2021   \n",
       "17      Aas, Kjersti, Jullum, Martin, Løland, Anders       September 2021   \n",
       "18  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...           June 2021   \n",
       "19    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.        December 2016   \n",
       "20                      Riveiro, Maria, Thill, Serge       September 2021   \n",
       "21  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...            May 2021   \n",
       "22  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...        January 2014   \n",
       "23                      Kohavi, Ron, John, George H.        December 1997   \n",
       "24  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...        October 2021   \n",
       "25                                   Ying, Mingsheng        February 2010   \n",
       "\n",
       "                                          Paper Links  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  \n",
       "25  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days. \n",
    "\n",
    "#Scrape below mentioned details :\n",
    "#i) Paper Title \n",
    "#ii) Authors\n",
    "#iii) Published Date \n",
    "#iv) Paper URL \n",
    "page=requests.get(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")\n",
    "page   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup= BeautifulSoup(page8.content)\n",
    "\n",
    "paper_title=[]           #To store the paper titles\n",
    "for i in soup.find_all('h2',class_=\"sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR\"):\n",
    "    paper_title.append(i.text)\n",
    "\n",
    "authors=[]               #To store the author names\n",
    "for i in soup.find_all('span',class_=\"sc-1w3fpd7-0 pgLAT\"):\n",
    "    authors.append(i.text)\n",
    "    \n",
    "date=[]                  #To store the publication date\n",
    "for i in soup.find_all('span',class_=\"sc-1thf9ly-2 bKddwo\"):\n",
    "    date.append(i.text)\n",
    "\n",
    "url=[]                   #To store the urls\n",
    "for i in soup.find_all('a',class_=\"sc-5smygv-0 nrDZj\"):\n",
    "    url.append(i.get('href'))\n",
    "\n",
    "l=len(paper_title)\n",
    "s_no=[]\n",
    "for j in range(1,l+1):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df=pd.DataFrame({'Paper Titles':paper_title, 'Authors':authors, 'Date of Publication':date, 'Paper Links':url}, index=s_no)\n",
    "print(\"The details of most downloaded articles from AI in last 90 days\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694b8b9",
   "metadata": {},
   "source": [
    "# 9) Write a python program to scrape mentioned details from dineout.co.in :\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location \n",
    "iv) Ratings\n",
    "v) Image URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "812e8321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The details from dineout.co.in\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Image URL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Castle Barbeque</th>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jungle Jamboree</th>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Castle Barbeque</th>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cafe Knosh</th>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Barbeque Company</th>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>India Grill</th>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delhi Barbeque</th>\n",
       "      <td>North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Monarch - Bar Be Que Village</th>\n",
       "      <td>North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indian Grill Room</th>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Cuisine  \\\n",
       "Restaurant Name                                                  \n",
       "Castle Barbeque                          Chinese, North Indian   \n",
       "Jungle Jamboree                   North Indian, Asian, Italian   \n",
       "Castle Barbeque                          Chinese, North Indian   \n",
       "Cafe Knosh                                Italian, Continental   \n",
       "The Barbeque Company                     North Indian, Chinese   \n",
       "India Grill                              North Indian, Italian   \n",
       "Delhi Barbeque                                    North Indian   \n",
       "The Monarch - Bar Be Que Village                  North Indian   \n",
       "Indian Grill Room                        North Indian, Mughlai   \n",
       "\n",
       "                                                                           Location  \\\n",
       "Restaurant Name                                                                       \n",
       "Castle Barbeque                                      Connaught Place, Central Delhi   \n",
       "Jungle Jamboree                              3CS Mall,Lajpat Nagar - 3, South Delhi   \n",
       "Castle Barbeque                              Pacific Mall,Tagore Garden, West Delhi   \n",
       "Cafe Knosh                        The Leela Ambience Convention Hotel,Shahdara, ...   \n",
       "The Barbeque Company                             Gardens Galleria,Sector 38A, Noida   \n",
       "India Grill                                    Hilton Garden Inn,Saket, South Delhi   \n",
       "Delhi Barbeque                       Taurus Sarovar Portico,Mahipalpur, South Delhi   \n",
       "The Monarch - Bar Be Que Village  Indirapuram Habitat Centre,Indirapuram, Ghaziabad   \n",
       "Indian Grill Room                  Suncity Business Tower,Golf Course Road, Gurgaon   \n",
       "\n",
       "                                 Ratings  \\\n",
       "Restaurant Name                            \n",
       "Castle Barbeque                      4.1   \n",
       "Jungle Jamboree                      3.9   \n",
       "Castle Barbeque                      3.9   \n",
       "Cafe Knosh                           4.3   \n",
       "The Barbeque Company                   4   \n",
       "India Grill                          3.9   \n",
       "Delhi Barbeque                       3.6   \n",
       "The Monarch - Bar Be Que Village     3.8   \n",
       "Indian Grill Room                    4.3   \n",
       "\n",
       "                                                                          Image URL  \n",
       "Restaurant Name                                                                      \n",
       "Castle Barbeque                   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "Jungle Jamboree                   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "Castle Barbeque                   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "Cafe Knosh                        https://im1.dineout.co.in/images/uploads/resta...  \n",
       "The Barbeque Company              https://im1.dineout.co.in/images/uploads/resta...  \n",
       "India Grill                       https://im1.dineout.co.in/images/uploads/resta...  \n",
       "Delhi Barbeque                    https://im1.dineout.co.in/images/uploads/resta...  \n",
       "The Monarch - Bar Be Que Village  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "Indian Grill Room                 https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "page=requests.get(\"https://www.dineout.co.in/delhi-restaurants/buffet-special\")\n",
    "page   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup= BeautifulSoup(page9.content)\n",
    "\n",
    "r_name=[]           #To store the restaurant name\n",
    "for i in soup.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "    r_name.append(i.text)\n",
    "\n",
    "cuisine=[]           #To store the cuisine\n",
    "for i in soup.find_all('span',class_=\"double-line-ellipsis\"):\n",
    "    cuisine.append(' '.join(i.text.split()[6:]))\n",
    "cuisine\n",
    "\n",
    "location=[]         #To store location\n",
    "for i in soup.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "    location.append(i.text)\n",
    "location\n",
    "\n",
    "ratings=[]          #To store ratings\n",
    "for i in soup.find_all('div',class_=\"restnt-rating rating-4\"):\n",
    "    ratings.append(i.text)\n",
    "\n",
    "images=[]           #to store images\n",
    "for i in soup.find_all('img',class_=\"no-img\"):\n",
    "    images.append(i['data-src'])\n",
    "    \n",
    "images\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df=pd.DataFrame({'Restaurant Name':r_name, 'Cuisine':cuisine, 'Location':location, 'Ratings':ratings, 'Image URL':images})\n",
    "print(\"The details from dineout.co.in\")\n",
    "df.set_index('Restaurant Name', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654499c",
   "metadata": {},
   "source": [
    "# 10 Write a python program to scrape the details of top publications from Google Scholar from \n",
    "#https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "i) Rank \n",
    "ii) Publication\n",
    "iii) h5-index\n",
    " iv) h5-median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25924563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Publication</th>\n",
       "      <th>h5-index</th>\n",
       "      <th>h5-median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>432</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Science</td>\n",
       "      <td>401</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>389</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>354</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>145</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>145</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>144</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>144</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                        Publication h5-index h5-median\n",
       "0     1.                                             Nature      444       667\n",
       "1     2.                The New England Journal of Medicine      432       780\n",
       "2     3.                                            Science      401       614\n",
       "3     4.  IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
       "4     5.                                         The Lancet      354       635\n",
       "..   ...                                                ...      ...       ...\n",
       "95   96.                       Journal of Business Research      145       233\n",
       "96   97.                                   Molecular Cancer      145       209\n",
       "97   98.                                            Sensors      145       201\n",
       "98   99.                              Nature Climate Change      144       228\n",
       "99  100.                    IEEE Internet of Things Journal      144       212\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = requests.get('https://scholar.google.com/citations?view_op=top_venues&hl=en') # Send request to webserver to get the source code\n",
    "\n",
    "soup = BeautifulSoup(page.content) # To Assign the page content to the variable soup\n",
    "soup\n",
    "\n",
    "publications = []\n",
    "rank = []\n",
    "h5index = []\n",
    "h5median = []\n",
    "\n",
    "# Assigning publication name to the variable\n",
    "for title in soup.find_all('td', class_='gsc_mvt_t'):\n",
    "    publications.append(title.text)\n",
    "publications\n",
    "\n",
    "# Assigning rank to the variable\n",
    "for title in soup.find_all('td', class_='gsc_mvt_p'):\n",
    "    rank.append(title.text)\n",
    "rank\n",
    "\n",
    "# Assigning h5_index to the variable\n",
    "for title in soup.find_all('a', class_='gs_ibl gsc_mp_anchor'):\n",
    "    h5index.append(title.text)\n",
    "h5index\n",
    "\n",
    "# Assigning h5_median to the variable\n",
    "for title in soup.find_all('span', class_='gs_ibl gsc_mp_anchor'):\n",
    "    h5median.append(title.text)\n",
    "h5median\n",
    "\n",
    "# Assigning data to the Dataframe\n",
    "import pandas as pd\n",
    "data = pd.DataFrame()\n",
    "data['Rank'] = rank\n",
    "data['Publication'] = publications\n",
    "data['h5-index'] = h5index\n",
    "data['h5-median'] = h5median\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba79253c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48e889de",
   "metadata": {},
   "source": [
    "6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "c) Top 10 ODI bowlers along with the records of their team and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d1fdb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in women’s cricket along with the records for matches, points and rating\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australia</td>\n",
       "      <td>18</td>\n",
       "      <td>3,061</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>26</td>\n",
       "      <td>3,098</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>England</td>\n",
       "      <td>25</td>\n",
       "      <td>2,904</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>India</td>\n",
       "      <td>27</td>\n",
       "      <td>2,820</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>24</td>\n",
       "      <td>2,425</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>24</td>\n",
       "      <td>2,334</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>12</td>\n",
       "      <td>932</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>21</td>\n",
       "      <td>1,237</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>11</td>\n",
       "      <td>516</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>8</td>\n",
       "      <td>353</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Team Matches Points Ratings\n",
       "1      Australia      18  3,061     170\n",
       "2   South Africa      26  3,098     119\n",
       "3        England      25  2,904     116\n",
       "4          India      27  2,820     104\n",
       "5    New Zealand      24  2,425     101\n",
       "6    West Indies      24  2,334      97\n",
       "7     Bangladesh      12    932      78\n",
       "8       Pakistan      21  1,237      59\n",
       "9        Ireland      11    516      47\n",
       "10     Sri Lanka       8    353      44"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page6a=requests.get(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")\n",
    "page6a   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup6a= BeautifulSoup(page6a.content)\n",
    "\n",
    "a=soup6a.find('tr',class_=\"rankings-block__banner\")    #Store the highest ranker\n",
    "t=[]\n",
    "m=[]\n",
    "p=[]\n",
    "r=[]\n",
    "l=len(a.text.split())\n",
    "for j in range(1,l):\n",
    "    b=a.text.split()[j]\n",
    "    if b[0]=='1' or b[0]=='2' or b[0]=='3' or b[0]=='4' or b[0]=='5' or b[0]=='6' or b[0]=='7' or b[0]=='8' or b[0]=='9':\n",
    "        k=j\n",
    "        break\n",
    "t.append(' '.join(a.text.split()[1:k-1]))\n",
    "m.append(''.join(a.text.split()[k]))\n",
    "p.append(''.join(a.text.split()[k+1]))\n",
    "r.append(''.join(a.text.split()[k+2]))\n",
    "\n",
    "z=1                                #Counter to get only top 10\n",
    "for i in soup6a.find_all('tr',class_=\"table-body\"):\n",
    "    if z<=9:\n",
    "        l=len(i.text.split())\n",
    "        for j in range(1,l):\n",
    "            b=i.text.split()[j]\n",
    "            if b[0]=='1' or b[0]=='2' or b[0]=='3' or b[0]=='4' or b[0]=='5' or b[0]=='6' or b[0]=='7' or b[0]=='8' or b[0]=='9':\n",
    "                k=j\n",
    "                break\n",
    "        t.append(' '.join(i.text.split()[1:k-1]))\n",
    "        m.append(''.join(i.text.split()[k]))\n",
    "        p.append(''.join(i.text.split()[k+1]))\n",
    "        r.append(''.join(i.text.split()[k+2]))\n",
    "        z=z+1\n",
    "              \n",
    "s_no=[]\n",
    "for j in range(1,11):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df6a=pd.DataFrame({'Team':t, 'Matches':m, 'Points':p, 'Ratings':r}, index=s_no)\n",
    "print(\"Top 10 ODI teams in women’s cricket along with the records for matches, points and rating\")\n",
    "df6a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84f8a075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 women’s ODI Batting players along with the records of their team and rating.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batting Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Harmanpreet Kaur</td>\n",
       "      <td>IND</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rachael Haynes</td>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chamari Athapaththu</td>\n",
       "      <td>SL</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Batting Player Team Rating\n",
       "1          Alyssa Healy  AUS    785\n",
       "2           Beth Mooney  AUS    749\n",
       "3       Laura Wolvaardt   SA    732\n",
       "4        Natalie Sciver  ENG    725\n",
       "5      Harmanpreet Kaur  IND    716\n",
       "6       Smriti Mandhana  IND    714\n",
       "7           Meg Lanning  AUS    710\n",
       "8        Rachael Haynes  AUS    701\n",
       "9     Amy Satterthwaite   NZ    661\n",
       "10  Chamari Athapaththu   SL    655"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page6b=requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\")\n",
    "page6b   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup6b= BeautifulSoup(page6b.content)\n",
    "\n",
    "a=soup6b.find('tr',class_=\"rankings-block__banner\")    #Store the highest ranker\n",
    "p=[]\n",
    "t=[]\n",
    "r=[]\n",
    "l=len(a.text.split())\n",
    "for j in range(2,l):\n",
    "    b=a.text.split()[j]\n",
    "    if b[0]=='9' or b[0]=='8' or b[0]=='7' or b[0]=='6' or b[0]=='5' or b[0]=='4' or b[0]=='1':\n",
    "        k=j\n",
    "        break\n",
    "if a.text.split()[1]=='(0)':\n",
    "    p.append(' '.join(a.text.split()[2:k-1]))\n",
    "    t.append(''.join(a.text.split()[k-1]))\n",
    "    r.append(''.join(a.text.split()[k]))\n",
    "else:\n",
    "    p.append(' '.join(a.text.split()[15:k-1]))\n",
    "    t.append(''.join(a.text.split()[k-1]))\n",
    "    r.append(''.join(a.text.split()[k]))\n",
    "\n",
    "z=1                                #Counter to get only top 10\n",
    "for i in soup6b.find_all('tr',class_=\"table-body\"):\n",
    "    if z<=9:\n",
    "        l=len(i.text.split())\n",
    "        for j in range(2,l):\n",
    "            b=i.text.split()[j]\n",
    "            if b[0]=='9' or b[0]=='8' or b[0]=='7' or b[0]=='6' or b[0]=='5' or b[0]=='4' or b[0]=='1':\n",
    "                k=j\n",
    "                break\n",
    "        if i.text.split()[1]=='(0)':\n",
    "            p.append(' '.join(i.text.split()[2:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        else:\n",
    "            p.append(' '.join(i.text.split()[15:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        z=z+1\n",
    "              \n",
    "s_no=[]\n",
    "for j in range(1,11):\n",
    "    s_no.append(j)\n",
    "    \n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df6b=pd.DataFrame({'Batting Player':p, 'Team':t, 'Rating':r}, index=s_no)\n",
    "print(\"Top 10 women’s ODI Batting players along with the records of their team and rating.\")\n",
    "df6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "946e5b82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 women’s ODI all-rounder along with the records of their team and rating.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All Rounder</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jhulan Goswami</td>\n",
       "      <td>IND</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Katherine Brunt</td>\n",
       "      <td>ENG</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         All Rounder Team Rating\n",
       "1    Hayley Matthews   WI    380\n",
       "2       Ellyse Perry  AUS    374\n",
       "3     Natalie Sciver  ENG    357\n",
       "4        Amelia Kerr   NZ    356\n",
       "5     Marizanne Kapp   SA    349\n",
       "6      Deepti Sharma  IND    322\n",
       "7   Ashleigh Gardner  AUS    270\n",
       "8      Jess Jonassen  AUS    246\n",
       "9     Jhulan Goswami  IND    214\n",
       "10   Katherine Brunt  ENG    207"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page6c=requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\")\n",
    "page6c   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup6c= BeautifulSoup(page6c.content)\n",
    "\n",
    "a=soup6c.find('tr',class_=\"rankings-block__banner\")    #Store the highest ranker\n",
    "p=[]\n",
    "t=[]\n",
    "r=[]\n",
    "l=len(a.text.split())\n",
    "for j in range(2,l):\n",
    "    b=a.text.split()[j]\n",
    "    if b[0]=='1' or b[0]=='2' or b[0]=='3'  or b[0]=='4' or b[0]=='5' or b[0]=='6' or b[0]=='7' or b[0]=='8' or b[0]=='9':\n",
    "        k=j\n",
    "        break\n",
    "if a.text.split()[1]=='(0)':\n",
    "    p.append(' '.join(a.text.split()[2:k-1]))\n",
    "    t.append(''.join(a.text.split()[k-1]))\n",
    "    r.append(''.join(a.text.split()[k]))\n",
    "else:\n",
    "    p.append(' '.join(a.text.split()[15:k-1]))\n",
    "    t.append(''.join(a.text.split()[k-1]))\n",
    "    r.append(''.join(a.text.split()[k]))\n",
    "\n",
    "z=1                                #Counter to get only top 10\n",
    "for i in soup6c.find_all('tr',class_=\"table-body\"):\n",
    "    if z<=9:\n",
    "        l=len(i.text.split())\n",
    "        for j in range(2,l):\n",
    "            b=i.text.split()[j]\n",
    "            if b[0]=='1' or b[0]=='2' or b[0]=='3'  or b[0]=='4' or b[0]=='5' or b[0]=='6' or b[0]=='7' or b[0]=='8' or b[0]=='9':\n",
    "                k=j\n",
    "                break\n",
    "        if i.text.split()[1]=='(0)':\n",
    "            p.append(' '.join(i.text.split()[2:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        else:\n",
    "            p.append(' '.join(i.text.split()[15:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        z=z+1\n",
    "              \n",
    "s_no=[]\n",
    "for j in range(1,11):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df6c=pd.DataFrame({'All Rounder':p, 'Team':t, 'Rating':r}, index=s_no)\n",
    "print(\"Top 10 women’s ODI all-rounder along with the records of their team and rating.\")\n",
    "df6c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fbbcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "c) Top 10 ODI bowlers along with the records of their team and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "647ea22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>England</td>\n",
       "      <td>27</td>\n",
       "      <td>3,226</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>22</td>\n",
       "      <td>2,508</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>34</td>\n",
       "      <td>3,802</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>22</td>\n",
       "      <td>2,354</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Australia</td>\n",
       "      <td>29</td>\n",
       "      <td>3,071</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>24</td>\n",
       "      <td>2,392</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>30</td>\n",
       "      <td>2,753</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>29</td>\n",
       "      <td>2,658</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>41</td>\n",
       "      <td>2,902</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>18</td>\n",
       "      <td>1,238</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Team Matches Points Ratings\n",
       "1        England      27  3,226     119\n",
       "2    New Zealand      22  2,508     114\n",
       "3          India      34  3,802     112\n",
       "4       Pakistan      22  2,354     107\n",
       "5      Australia      29  3,071     106\n",
       "6   South Africa      24  2,392     100\n",
       "7     Bangladesh      30  2,753      92\n",
       "8      Sri Lanka      29  2,658      92\n",
       "9    West Indies      41  2,902      71\n",
       "10   Afghanistan      18  1,238      69"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5a. Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape: a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating. \n",
    "\n",
    "page5a=requests.get(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")\n",
    "page5a   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup5a= BeautifulSoup(page5a.content)\n",
    "\n",
    "a=soup5a.find('tr',class_=\"rankings-block__banner\")    #Store the highest ranker\n",
    "t=[]\n",
    "m=[]\n",
    "p=[]\n",
    "r=[]\n",
    "l=len(a.text.split())\n",
    "for j in range(1,l):\n",
    "    b=a.text.split()[j]\n",
    "    if b[0]=='1' or b[0]=='2' or b[0]=='3' or b[0]=='4' or b[0]=='5':\n",
    "        k=j\n",
    "        break\n",
    "t.append(' '.join(a.text.split()[1:k-1]))\n",
    "m.append(''.join(a.text.split()[k]))\n",
    "p.append(''.join(a.text.split()[k+1]))\n",
    "r.append(''.join(a.text.split()[k+2]))\n",
    "\n",
    "z=1                                #Counter to get only top 10\n",
    "for i in soup5a.find_all('tr',class_=\"table-body\"):\n",
    "    if z<=9:\n",
    "        l=len(i.text.split())\n",
    "        for j in range(1,l):\n",
    "            b=i.text.split()[j]\n",
    "            if b[0]=='1' or b[0]=='2' or b[0]=='3' or b[0]=='4' or b[0]=='5':\n",
    "                k=j\n",
    "                break\n",
    "        t.append(' '.join(i.text.split()[1:k-1]))\n",
    "        m.append(''.join(i.text.split()[k]))\n",
    "        p.append(''.join(i.text.split()[k+1]))\n",
    "        r.append(''.join(i.text.split()[k+2]))\n",
    "        z=z+1\n",
    "              \n",
    "s_no=[]\n",
    "for j in range(1,11):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df5a=pd.DataFrame({'Team':t, 'Matches':m, 'Points':p, 'Ratings':r}, index=s_no)\n",
    "print(\"Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\")\n",
    "df5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98992869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Batsmen along with the records of their team and rating.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batsmen</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ross Taylor</td>\n",
       "      <td>NZ</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Steve Smith AUS</td>\n",
       "      <td>697</td>\n",
       "      <td>752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Batsmen Team Rating\n",
       "1              Babar Azam  PAK    890\n",
       "2             Imam-ul-Haq  PAK    779\n",
       "3   Rassie van der Dussen   SA    766\n",
       "4         Quinton de Kock   SA    759\n",
       "5          Jonny Bairstow  ENG    732\n",
       "6            David Warner  AUS    725\n",
       "7             Virat Kohli  IND    722\n",
       "8            Rohit Sharma  IND    718\n",
       "9             Ross Taylor   NZ    701\n",
       "10        Steve Smith AUS  697    752"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page5b=requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\")\n",
    "page5b   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup5b= BeautifulSoup(page5b.content)\n",
    "\n",
    "a=soup5b.find('tr',class_=\"rankings-block__banner\")    #Store the highest ranker\n",
    "p=[]\n",
    "t=[]\n",
    "r=[]\n",
    "l=len(a.text.split())\n",
    "for j in range(2,l):\n",
    "    b=a.text.split()[j]\n",
    "    if b[0]=='9' or b[0]=='8' or b[0]=='7' or b[0]=='1':\n",
    "        k=j\n",
    "        break\n",
    "p.append(' '.join(a.text.split()[2:k-1]))\n",
    "t.append(''.join(a.text.split()[k-1]))\n",
    "r.append(''.join(a.text.split()[k]))\n",
    "\n",
    "z=1                                #Counter to get only top 10\n",
    "for i in soup5b.find_all('tr',class_=\"table-body\"):\n",
    "    if z<=9:\n",
    "        l=len(i.text.split())\n",
    "        for j in range(2,l):\n",
    "            b=i.text.split()[j]\n",
    "            if b[0]=='9' or b[0]=='8' or b[0]=='7' or b[0]=='1':\n",
    "                k=j\n",
    "                break\n",
    "        if i.text.split()[1]=='(0)':\n",
    "            p.append(' '.join(i.text.split()[2:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        else:\n",
    "            p.append(' '.join(i.text.split()[15:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        z=z+1\n",
    "              \n",
    "s_no=[]\n",
    "for j in range(1,11):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df5b=pd.DataFrame({'Batsmen':p, 'Team':t, 'Rating':r}, index=s_no)\n",
    "print(\"Top 10 ODI Batsmen along with the records of their team and rating.\")\n",
    "df5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a28fdd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Bowlers along with the records of their team and rating.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bowler</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mohammad Nabi</td>\n",
       "      <td>AFG</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mehedi Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mitchell Starc</td>\n",
       "      <td>AUS</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rashid Khan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jasprit Bumrah</td>\n",
       "      <td>IND</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Bowler Team Rating\n",
       "1        Trent Boult   NZ    775\n",
       "2     Josh Hazlewood  AUS    718\n",
       "3   Mujeeb Ur Rahman  AFG    676\n",
       "4     Shaheen Afridi  PAK    661\n",
       "5      Mohammad Nabi  AFG    657\n",
       "6       Mehedi Hasan  BAN    655\n",
       "7         Matt Henry   NZ    654\n",
       "8     Mitchell Starc  AUS    653\n",
       "9        Rashid Khan  AFG    651\n",
       "10    Jasprit Bumrah  IND    642"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page5c=requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")\n",
    "page5c   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup5c= BeautifulSoup(page5c.content)\n",
    "\n",
    "a=soup5c.find('tr',class_=\"rankings-block__banner\")    #Store the highest ranker\n",
    "p=[]\n",
    "t=[]\n",
    "r=[]\n",
    "l=len(a.text.split())\n",
    "for j in range(2,l):\n",
    "    b=a.text.split()[j]\n",
    "    if b[0]=='9' or b[0]=='8' or b[0]=='7' or b[0]=='6' or b[0]=='1':\n",
    "        k=j\n",
    "        break\n",
    "p.append(' '.join(a.text.split()[2:k-1]))\n",
    "t.append(''.join(a.text.split()[k-1]))\n",
    "r.append(''.join(a.text.split()[k]))\n",
    "\n",
    "z=1                                #Counter to get only top 10\n",
    "for i in soup5c.find_all('tr',class_=\"table-body\"):\n",
    "    if z<=9:\n",
    "        l=len(i.text.split())\n",
    "        for j in range(2,l):\n",
    "            b=i.text.split()[j]\n",
    "            if b[0]=='9' or b[0]=='8' or b[0]=='7' or b[0]=='6' or b[0]=='1':\n",
    "                k=j\n",
    "                break\n",
    "        if i.text.split()[1]=='(0)':\n",
    "            p.append(' '.join(i.text.split()[2:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        else:\n",
    "            p.append(' '.join(i.text.split()[15:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        z=z+1\n",
    "              \n",
    "s_no=[]\n",
    "for j in range(1,11):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df5c=pd.DataFrame({'Bowler':p, 'Team':t, 'Rating':r}, index=s_no)\n",
    "print(\"Top 10 ODI Bowlers along with the records of their team and rating.\")\n",
    "df5c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af6ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
